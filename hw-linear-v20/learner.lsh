(libload "dataset.lsh")

(defclass learner object
  my-data ; dataset object
  weights ; weight vector of the current best decision hyperplane
  iteration-limit;
  l1-regularizer-c;
  l2-regularizer-c;
  precision;
  )

(defmethod learner learner (dataset)
  (setq my-data dataset)
  (setq weights (double-matrix (idx-dim :my-data:inputs-train 1)))
  (setq weights (idx-copy (select :data:inputs-train 0 (rand 0 (idx-dim :data:outputs-train 0)))))
  (setq iteration-limit 100000000)
  (setq precision 0.000001)
  (setq l1-regularizer-c [@ 0])
  (setq l2-regularizer-c [@ 1000]))

  (defmethod learner name () "Abstract")

  (defmethod learner run ()
  (let ((iterations (==> this solve)))
    (if (= iterations ())
      (print (concat (==> this name) " learning procedure did not converge in " (str iteration-limit) " iterations."))
      (print (concat (==> this name) " learning procedure complete after " (str iterations) " iterations.  Result:")))
    (==> this print-error)
  ))

(defmethod learner print-error ()
  (print (concat "Training set error: " (str (==> this error :my-data:inputs-train :my-data:outputs-train))))
  (print (concat "Training set average loss: " (str (==> this train-loss))))
  (print (concat "Test set error: " (str (==> this error :my-data:inputs-test :my-data:outputs-test))))
  (print (concat "Test set average loss: " (str (==> this test-loss)))))

#? (==> <learner> threshold-f <vector1>)
;; Calculate the dot product of vector1 and weights
;; Return 1 if the dot-product is > 0
;; return -1 otherwise
(defmethod learner threshold-f (Xp)
    (if (> ((idx-dot weights Xp)) 0)
      1
      -1))

(defmethod learner error (input-matrix output-vector)
 (let ((errors 0))
    (idx-bloop ((row input-matrix) (output output-vector))
      (if (<> (output) (==> this threshold-f row))
        (setq errors (+ errors 1))))
     (/ errors (idx-dim output-vector 0))))

(defmethod learner loss (input-matrix output-vector)
  (let ((sum 0))
    (idx-bloop ((row input-matrix) (output output-vector))
      (setq sum (+ sum (==> this per-sample-loss row output))))
     (+ 
        (/ sum (idx-dim output-vector 0))
        ;((==> this l1-regularizer))
        ((==> this l2-regularizer)))
        ))

(defmethod learner test-loss ()
  (==> this loss :my-data:inputs-test :my-data:outputs-test))

(defmethod learner train-loss ()
  (==> this loss :my-data:inputs-train :my-data:outputs-train))

(defmethod learner solve-by-gradient-descent()
  (let ((counter 0) (converged ())) 
    (while (and (< counter (==> this full-iteration-limit)) (not converged))
      (setq converged (==> this iterate-weights-once (==> this step-size counter)))
      (setq counter (+ counter 1)))
    (if converged (* counter (idx-dim :my-data:outputs-train 0)) () )));; If converged, return counter, else return ()

(defmethod learner iterate-weights-once (step-size)
    (let ((converged t))
    (idx-bloop ((row :my-data:inputs-train) (output :my-data:outputs-train))
        (setq converged (and converged 
                             (==> this iterate-weights-for-row row output step-size))))
    converged))

;; W(t+1) = W(t) + eta(t)(-1*gradient)
(defmethod learner iterate-weights-for-row (row output step-size)
  (let* ((weight-increment (idx-dotm0 (==> this gradient row output) step-size)))
    (==> this l2-weight-decay step-size)
    ;(==> this l1-weight-decay step-size)
    (setq weights (idx-add weights weight-increment))
    (==> this converged? weight-increment)))

(defmethod learner l2-weight-decay (step-size)
  (setq weights 
    (idx-dotm0 weights (idx-sub [@ 1] (idx-mul step-size l2-regularizer-c))))); l2 Regularization

(defmethod learner l1-weight-decay (step-size)
  (idx-bloop ((weight weights))
    (weight ((idx-add weight (idx-mul step-size l1-regularizer-c))))))

(defmethod learner converged? (weight-increment)
  ;(if (> ((idx-sup weight-increment)) precision) (print (idx-sup weight-increment)))
  (< ((idx-sup weight-increment)) precision))

(defmethod learner l1-regularizer ()
  (idx-mul l1-regularizer-c (idx-sum weights)))
(defmethod learner l2-regularizer ()
  (idx-mul (idx-mul l2-regularizer-c [@ .5]) (idx-sumsqr weights)))

(defmethod learner step-size (iteration-number)
  (let ((c1 .001)(c2 50))
    (scalar (/ c1 (+ c2 iteration-number)))))

(defmethod learner full-iteration-limit ()
  (/ iteration-limit (idx-dim :my-data:outputs-train 0)))
