(libload "dataset.lsh")

(defclass learner object
  my-data ; dataset object
  weights ; weight vector of the current best decision hyperplane
  iteration-limit;
  l1-regularizer-c;
  l2-regularizer-c;
  )

(defmethod learner learner (dataset)
  (setq my-data dataset)
  (setq weights (double-matrix (idx-dim :my-data:inputs-train 1)))
  (setq iteration-limit 10000)
  (setq l1-regularizer-c [@ 0])
  (setq l2-regularizer-c [@ .00001]))

  (defmethod learner name () "Abstract")

  (defmethod learner run ()
  (let ((iterations (==> this solve)))
    (if (= iterations ())
      (print (concat (==> this name) " learning procedure did not converge in " (str iteration-limit) " iterations."))
      (print (concat (==> this name) " learning procedure complete after " (str iterations) " iterations.  Result:")))
    (==> this print-error)
  ))

(defmethod learner print-error ()
  (print (concat "Training set error: " (str (==> this error :my-data:inputs-train :my-data:outputs-train))))
  (print (concat "Training set average loss: " (str (==> this train-loss))))
  (print (concat "Test set error: " (str (==> this error :my-data:inputs-test :my-data:outputs-test))))
  (print (concat "Test set average loss: " (str (==> this test-loss)))))

#? (==> <learner> threshold-f <vector1>)
;; Calculate the dot product of vector1 and weights
;; Return 1 if the dot-product is > 0
;; return -1 otherwise
(defmethod learner threshold-f (Xp)
    (if (> ((idx-dot weights Xp)) 0)
      1
      -1))

(defmethod learner error (input-matrix output-vector)
 (let ((sum 0))
    (idx-bloop ((row input-matrix) (output output-vector))
      (setq sum (+ sum (abs (- (output) (==> this threshold-f row))))))
     (/ sum (idx-dim input-matrix 0))))

(defmethod learner loss (input-matrix output-vector)
  (let ((sum 0))
    (idx-bloop ((row input-matrix) (output output-vector))
      (setq sum (+ sum (==> this per-sample-loss row output))))
     (+ (/ sum (idx-dim output-vector 0))
        ((==> this l1-regularizer))
        ((==> this l2-regularizer)))))

(defmethod learner test-loss ()
  (==> this loss :my-data:inputs-test :my-data:outputs-test))

(defmethod learner train-loss ()
  (==> this loss :my-data:inputs-train :my-data:outputs-train))

(defmethod learner solve-by-gradient-descent()
  (let ((counter 0) (converged ())) 
    (while (and (< counter iteration-limit) (not converged))
      (setq converged (==> this iterate-weights-once (==> this step-size counter)))
      (setq counter (+ counter 1)))
    (if converged counter ())));; If converged, return counter, else return empty

(defmethod learner iterate-weights-once (step-size)
    (let ((converged t))
    (idx-bloop ((row :my-data:inputs-train) (output :my-data:outputs-train))
        (setq converged (and converged 
                             (==> this iterate-weights-for-row row output step-size))))
    converged))

;; W(t+1) = W(t) + eta(t)(-1*gradient)
(defmethod learner iterate-weights-for-row (row output step-size)
  (let* ((weight-increment (idx-dotm0 (==> this gradient row output) step-size))
         (old-loss (==> this test-loss)))
    ;(setq weights (idx-sub weights (idx-dotm0 weights (idx-sub [@ 1] (idx-mul step-size l2-regularizer-c))))); l2 Regularization
    (setq weights (idx-add weights weight-increment))
    (==> this converged? old-loss)))

(defmethod learner converged? (old-loss)
    (print (- old-loss (==> this train-loss)))
  (< (- old-loss (==> this train-loss)) 0.001))

(defmethod learner l1-regularizer ()
  (idx-mul l1-regularizer-c (idx-sum weights)))
(defmethod learner l2-regularizer ()
  (idx-mul (idx-mul l2-regularizer-c [@ .5]) (idx-sumsqr weights)))
